{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>...</td>\n",
       "      <td>259.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>320.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>...</td>\n",
       "      <td>213.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>...</td>\n",
       "      <td>122.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>...</td>\n",
       "      <td>152.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>242.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>...</td>\n",
       "      <td>212.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>257.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>...</td>\n",
       "      <td>260.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>257.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>258.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>257.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>259.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>...</td>\n",
       "      <td>258.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>257.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>...</td>\n",
       "      <td>258.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4998 rows Ã— 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4      5      6      7      8      9  \\\n",
       "0     189.0  191.0  192.0  197.0  208.0  219.0  229.0  237.0  245.0  244.0   \n",
       "1     320.0  351.0  365.0  390.0  404.0  417.0  429.0  427.0  422.0  410.0   \n",
       "2     241.0  238.0  234.0  234.0  227.0  223.0  221.0  222.0  223.0  224.0   \n",
       "3     306.0  298.0  295.0  286.0  284.0  275.0  275.0  271.0  266.0  264.0   \n",
       "4     242.0  248.0  240.0  241.0  236.0  231.0  230.0  232.0  233.0  237.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "4993  257.0  256.0  257.0  256.0  258.0  253.0  254.0  250.0  255.0  260.0   \n",
       "4994  257.0  257.0  258.0  257.0  257.0  251.0  245.0  267.0  263.0  249.0   \n",
       "4995  258.0  257.0  258.0  257.0  258.0  253.0  257.0  258.0  257.0  255.0   \n",
       "4996  259.0  257.0  258.0  257.0  257.0  254.0  260.0  255.0  255.0  259.0   \n",
       "4997  257.0  260.0  257.0  256.0  260.0  257.0  255.0  256.0  255.0  253.0   \n",
       "\n",
       "      ...    151    152    153    154    155    156    157    158    159  \\\n",
       "0     ...  259.0  217.0  176.0  148.0  140.0  148.0  164.0  175.0  154.0   \n",
       "1     ...  213.0  205.0  216.0  229.0  248.0  251.0  248.0  216.0  164.0   \n",
       "2     ...  122.0  137.0  153.0  168.0  190.0  203.0  221.0  238.0  253.0   \n",
       "3     ...  152.0  168.0  181.0  178.0  157.0  132.0  115.0   95.0   76.0   \n",
       "4     ...  212.0  198.0  175.0  162.0  141.0  120.0  119.0  144.0  178.0   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "4993  ...  260.0  259.0  257.0  256.0  257.0  258.0  257.0  260.0  255.0   \n",
       "4994  ...  255.0  255.0  260.0  256.0  256.0  257.0  256.0  254.0  256.0   \n",
       "4995  ...  257.0  258.0  258.0  256.0  260.0  259.0  254.0  256.0  258.0   \n",
       "4996  ...  258.0  258.0  260.0  257.0  256.0  257.0  256.0  258.0  258.0   \n",
       "4997  ...  258.0  258.0  260.0  256.0  257.0  260.0  257.0  259.0  260.0   \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "4993       0  \n",
       "4994       0  \n",
       "4995       0  \n",
       "4996       0  \n",
       "4997       0  \n",
       "\n",
       "[4998 rows x 161 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../preprocess/202111171547_all.csv')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = df.iloc[:, :-1]\n",
    "target = df['target']\n",
    "\n",
    "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, stratify=target, random_state=0)\n",
    "train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, stratify=train_target, random_state=0)\n",
    "\n",
    "# test = np.array((1, train_input.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 160, 16)           8000      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 8)                 800       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,809\n",
      "Trainable params: 8,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 12:12:29.018599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 12:12:29.024454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 12:12:29.024841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 12:12:29.025667: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-19 12:12:29.027093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 12:12:29.027512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 12:12:29.027935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 12:12:29.519051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 12:12:29.519390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 12:12:29.519698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 12:12:29.519993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2182 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(500, 16, input_length=160))\n",
    "model.add(keras.layers.LSTM(8, dropout=0.3))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 12:12:32.085207: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 4s 15ms/step - loss: 0.3788 - accuracy: 0.8374 - val_loss: 0.3094 - val_accuracy: 0.8913\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.3082 - accuracy: 0.8881 - val_loss: 0.2680 - val_accuracy: 0.8900\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2949 - accuracy: 0.8906 - val_loss: 0.2616 - val_accuracy: 0.9025\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2850 - accuracy: 0.8962 - val_loss: 0.2673 - val_accuracy: 0.8988\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.2680 - accuracy: 0.9040 - val_loss: 0.2866 - val_accuracy: 0.8838\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2589 - accuracy: 0.9081 - val_loss: 0.2798 - val_accuracy: 0.8950\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.2448 - accuracy: 0.9124 - val_loss: 0.2848 - val_accuracy: 0.8963\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2368 - accuracy: 0.9206 - val_loss: 0.3004 - val_accuracy: 0.9025\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2314 - accuracy: 0.9218 - val_loss: 0.2859 - val_accuracy: 0.8975\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2216 - accuracy: 0.9246 - val_loss: 0.3023 - val_accuracy: 0.8938\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2189 - accuracy: 0.9271 - val_loss: 0.2989 - val_accuracy: 0.8950\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2059 - accuracy: 0.9296 - val_loss: 0.2969 - val_accuracy: 0.9025\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2059 - accuracy: 0.9293 - val_loss: 0.3152 - val_accuracy: 0.8850\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1976 - accuracy: 0.9334 - val_loss: 0.3257 - val_accuracy: 0.8850\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1989 - accuracy: 0.9334 - val_loss: 0.3175 - val_accuracy: 0.8900\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1845 - accuracy: 0.9396 - val_loss: 0.3382 - val_accuracy: 0.8825\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1861 - accuracy: 0.9387 - val_loss: 0.3254 - val_accuracy: 0.8950\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1798 - accuracy: 0.9406 - val_loss: 0.3461 - val_accuracy: 0.8875\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1824 - accuracy: 0.9375 - val_loss: 0.3294 - val_accuracy: 0.8875\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1784 - accuracy: 0.9428 - val_loss: 0.3436 - val_accuracy: 0.8875\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1708 - accuracy: 0.9428 - val_loss: 0.3515 - val_accuracy: 0.8950\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1659 - accuracy: 0.9453 - val_loss: 0.3800 - val_accuracy: 0.8875\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1621 - accuracy: 0.9465 - val_loss: 0.3594 - val_accuracy: 0.8825\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1624 - accuracy: 0.9468 - val_loss: 0.3714 - val_accuracy: 0.8875\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1553 - accuracy: 0.9493 - val_loss: 0.3781 - val_accuracy: 0.8850\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1613 - accuracy: 0.9468 - val_loss: 0.3649 - val_accuracy: 0.8863\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1506 - accuracy: 0.9512 - val_loss: 0.3819 - val_accuracy: 0.8888\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1482 - accuracy: 0.9509 - val_loss: 0.3713 - val_accuracy: 0.9013\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1597 - accuracy: 0.9484 - val_loss: 0.4069 - val_accuracy: 0.8650\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1485 - accuracy: 0.9481 - val_loss: 0.3822 - val_accuracy: 0.8925\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1520 - accuracy: 0.9500 - val_loss: 0.3739 - val_accuracy: 0.8863\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1540 - accuracy: 0.9468 - val_loss: 0.3795 - val_accuracy: 0.8863\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1446 - accuracy: 0.9540 - val_loss: 0.3840 - val_accuracy: 0.8838\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1422 - accuracy: 0.9531 - val_loss: 0.4016 - val_accuracy: 0.8838\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1440 - accuracy: 0.9503 - val_loss: 0.3971 - val_accuracy: 0.8863\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1440 - accuracy: 0.9509 - val_loss: 0.3728 - val_accuracy: 0.8875\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1377 - accuracy: 0.9550 - val_loss: 0.4035 - val_accuracy: 0.8850\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1305 - accuracy: 0.9593 - val_loss: 0.4084 - val_accuracy: 0.8875\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1376 - accuracy: 0.9581 - val_loss: 0.4272 - val_accuracy: 0.8788\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1385 - accuracy: 0.9537 - val_loss: 0.3951 - val_accuracy: 0.8863\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1363 - accuracy: 0.9540 - val_loss: 0.3962 - val_accuracy: 0.8863\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1333 - accuracy: 0.9584 - val_loss: 0.4205 - val_accuracy: 0.8938\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1324 - accuracy: 0.9559 - val_loss: 0.4313 - val_accuracy: 0.8775\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1282 - accuracy: 0.9622 - val_loss: 0.4114 - val_accuracy: 0.8750\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1300 - accuracy: 0.9565 - val_loss: 0.4429 - val_accuracy: 0.8775\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1294 - accuracy: 0.9587 - val_loss: 0.4286 - val_accuracy: 0.8800\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1190 - accuracy: 0.9656 - val_loss: 0.4598 - val_accuracy: 0.8838\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1326 - accuracy: 0.9556 - val_loss: 0.4423 - val_accuracy: 0.8850\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1325 - accuracy: 0.9572 - val_loss: 0.4398 - val_accuracy: 0.8850\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1231 - accuracy: 0.9631 - val_loss: 0.4237 - val_accuracy: 0.8700\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1305 - accuracy: 0.9593 - val_loss: 0.4224 - val_accuracy: 0.8900\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1240 - accuracy: 0.9556 - val_loss: 0.4278 - val_accuracy: 0.8750\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1220 - accuracy: 0.9609 - val_loss: 0.4472 - val_accuracy: 0.8800\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1172 - accuracy: 0.9568 - val_loss: 0.4643 - val_accuracy: 0.8825\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1288 - accuracy: 0.9565 - val_loss: 0.4518 - val_accuracy: 0.8838\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1217 - accuracy: 0.9590 - val_loss: 0.4582 - val_accuracy: 0.8725\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1169 - accuracy: 0.9590 - val_loss: 0.4643 - val_accuracy: 0.8800\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1221 - accuracy: 0.9606 - val_loss: 0.4747 - val_accuracy: 0.8687\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1211 - accuracy: 0.9590 - val_loss: 0.4800 - val_accuracy: 0.8625\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1215 - accuracy: 0.9581 - val_loss: 0.4622 - val_accuracy: 0.8737\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1190 - accuracy: 0.9565 - val_loss: 0.4759 - val_accuracy: 0.8737\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1108 - accuracy: 0.9609 - val_loss: 0.4714 - val_accuracy: 0.8763\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1137 - accuracy: 0.9619 - val_loss: 0.4896 - val_accuracy: 0.8788\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1154 - accuracy: 0.9606 - val_loss: 0.4934 - val_accuracy: 0.8775\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1094 - accuracy: 0.9634 - val_loss: 0.4996 - val_accuracy: 0.8775\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1157 - accuracy: 0.9619 - val_loss: 0.5449 - val_accuracy: 0.8675\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1151 - accuracy: 0.9603 - val_loss: 0.5239 - val_accuracy: 0.8813\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1113 - accuracy: 0.9647 - val_loss: 0.5199 - val_accuracy: 0.8712\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1142 - accuracy: 0.9634 - val_loss: 0.5284 - val_accuracy: 0.8687\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1208 - accuracy: 0.9593 - val_loss: 0.5016 - val_accuracy: 0.8625\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1057 - accuracy: 0.9672 - val_loss: 0.5231 - val_accuracy: 0.8763\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1135 - accuracy: 0.9606 - val_loss: 0.5203 - val_accuracy: 0.8725\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1111 - accuracy: 0.9609 - val_loss: 0.5273 - val_accuracy: 0.8750\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1172 - accuracy: 0.9622 - val_loss: 0.5624 - val_accuracy: 0.8637\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1180 - accuracy: 0.9603 - val_loss: 0.5310 - val_accuracy: 0.8775\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1106 - accuracy: 0.9647 - val_loss: 0.5233 - val_accuracy: 0.8725\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1177 - accuracy: 0.9597 - val_loss: 0.5142 - val_accuracy: 0.8675\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1115 - accuracy: 0.9631 - val_loss: 0.5325 - val_accuracy: 0.8750\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1025 - accuracy: 0.9653 - val_loss: 0.5277 - val_accuracy: 0.8600\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1062 - accuracy: 0.9659 - val_loss: 0.5181 - val_accuracy: 0.8763\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1076 - accuracy: 0.9615 - val_loss: 0.4961 - val_accuracy: 0.8675\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1095 - accuracy: 0.9637 - val_loss: 0.4947 - val_accuracy: 0.8725\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1141 - accuracy: 0.9609 - val_loss: 0.5177 - val_accuracy: 0.8800\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1049 - accuracy: 0.9619 - val_loss: 0.4834 - val_accuracy: 0.8800\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1090 - accuracy: 0.9631 - val_loss: 0.5127 - val_accuracy: 0.8800\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1090 - accuracy: 0.9628 - val_loss: 0.5283 - val_accuracy: 0.8625\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1067 - accuracy: 0.9634 - val_loss: 0.5433 - val_accuracy: 0.8562\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0993 - accuracy: 0.9700 - val_loss: 0.5397 - val_accuracy: 0.8700\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1074 - accuracy: 0.9609 - val_loss: 0.5432 - val_accuracy: 0.8825\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1151 - accuracy: 0.9653 - val_loss: 0.5118 - val_accuracy: 0.8737\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0974 - accuracy: 0.9684 - val_loss: 0.5324 - val_accuracy: 0.8763\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1036 - accuracy: 0.9647 - val_loss: 0.5370 - val_accuracy: 0.8587\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1057 - accuracy: 0.9619 - val_loss: 0.5328 - val_accuracy: 0.8537\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1012 - accuracy: 0.9669 - val_loss: 0.5614 - val_accuracy: 0.8575\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1144 - accuracy: 0.9606 - val_loss: 0.5715 - val_accuracy: 0.8637\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1062 - accuracy: 0.9662 - val_loss: 0.5617 - val_accuracy: 0.8612\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1071 - accuracy: 0.9644 - val_loss: 0.5226 - val_accuracy: 0.8750\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1007 - accuracy: 0.9687 - val_loss: 0.5626 - val_accuracy: 0.8700\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1089 - accuracy: 0.9634 - val_loss: 0.5698 - val_accuracy: 0.8650\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1017 - accuracy: 0.9644 - val_loss: 0.5824 - val_accuracy: 0.8612\n"
     ]
    }
   ],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 5, restore_best_weights=True)\n",
    "history = model.fit(train_input, train_target, epochs=100, batch_size = 64,\n",
    "                    validation_data = (val_input, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e56d166fbc896341d80a58b1f49edfebe20bc75f207edfdf6ccda3bd4653dde"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('Senior_Fall_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
